{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras version: 2.3.1\n",
      "tensorflow version: 1.15.0\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "print(\"keras version:\", keras.__version__)\n",
    "import tensorflow as tf\n",
    "print(\"tensorflow version:\", tf.__version__)\n",
    "import wget\n",
    "import os\n",
    "import sys\n",
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "mean = np.mean(x_train,axis=(0,1,2,3))\n",
    "std = np.std(x_train, axis=(0, 1, 2, 3))\n",
    "x_train = (x_train-mean)/(std+1e-7)\n",
    "x_test = (x_test-mean)/(std+1e-7)\n",
    "x_valid = x_train[:10000]\n",
    "y_valid = y_train[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On model model_cifar10_balanced_seed-0_bestbefore-100_currentepoch-100_valacc-91_vgg.h5\n",
      "WARNING:tensorflow:From /Users/jsu/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /Users/jsu/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Making predictions on validation set\n",
      "WARNING:tensorflow:From /Users/jsu/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsu/opt/anaconda3/lib/python3.7/site-packages/keras/engine/saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      "/Users/jsu/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:23: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"co..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions on test set\n"
     ]
    }
   ],
   "source": [
    "model_files = [\n",
    "    \"model_cifar10_balanced_seed-0_bestbefore-100_currentepoch-100_valacc-91_vgg.h5\",\n",
    "    \"model_cifar10_balanced_seed-10_bestbefore-100_currentepoch-100_valacc-91_vgg.h5\",\n",
    "    \"model_cifar10_balanced_seed-20_bestbefore-100_currentepoch-100_valacc-91_vgg.h5\",\n",
    "    \"model_cifar10_balanced_seed-30_bestbefore-100_currentepoch-100_valacc-91_vgg.h5\",\n",
    "    \"model_cifar10_balanced_seed-40_bestbefore-100_currentepoch-100_valacc-91_vgg.h5\",\n",
    "    \"model_cifar10_balanced_seed-50_bestbefore-100_currentepoch-100_valacc-91_vgg.h5\",\n",
    "    \"model_cifar10_balanced_seed-60_bestbefore-100_currentepoch-100_valacc-91_vgg.h5\",\n",
    "    \"model_cifar10_balanced_seed-70_bestbefore-100_currentepoch-100_valacc-91_vgg.h5\",\n",
    "    \"model_cifar10_balanced_seed-80_bestbefore-100_currentepoch-100_valacc-90_vgg.h5\",\n",
    "    \"model_cifar10_balanced_seed-90_bestbefore-100_currentepoch-100_valacc-91_vgg.h5\"\n",
    "]\n",
    "model_file = model_files[0] # test with 1 model\n",
    "\n",
    "print(\"On model\", model_file)\n",
    "#   if (os.path.isfile(model_file)==False):\n",
    "#     print(\"Downloading\", model_file)\n",
    "#     wget.download(\"https://zenodo.org/record/2648107/files/\"\n",
    "#                   +model_file+\"?download=1\", out=model_file)\n",
    "model = load_model(\"models/\" + model_file)\n",
    "\n",
    "pre_softmax_model = Model(input=model.input,\n",
    "                        output=model.layers[-2].output)\n",
    "# print(\"Making predictions on validation set\")\n",
    "# valid_preacts = pre_softmax_model.predict(x_valid)\n",
    "# print(\"Making predictions on test set\")\n",
    "# test_preacts = pre_softmax_model.predict(x_test)\n",
    "\n",
    "# print(\"Test accuracy\",np.mean(np.argmax(test_preacts,axis=1)\n",
    "#                             == np.squeeze(y_test)))\n",
    "# print(\"Valid accuracy\",np.mean(np.argmax(valid_preacts,axis=1)\n",
    "#                              == np.squeeze(y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepexplain.tensorflow import DeepExplain\n",
    "from keras import backend as K\n",
    "\n",
    "with DeepExplain(session=K.get_session()) as de:  # <-- init DeepExplain context\n",
    "    # Need to reconstruct the graph in DeepExplain context, using the same weights.\n",
    "    # With Keras this is very easy:\n",
    "    # 1. Get the input tensor to the original model\n",
    "    input_tensor = model.input # model.layers[0].input\n",
    "    print(input_tensor)\n",
    "    \n",
    "    # 2. We now target the output of the last dense layer (pre-softmax)\n",
    "    # To do so, create a new model sharing the same layers untill the last dense (index -2)\n",
    "    fModel = Model(inputs=input_tensor, outputs = model.layers[-2].output)\n",
    "    target_tensor = fModel(input_tensor)\n",
    "    \n",
    "    print(target_tensor)\n",
    "    #y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    num_classes = 10\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "    xs = x_test[0:1]\n",
    "    ys = y_test[0:1]\n",
    "    \n",
    "    print(np.array(xs).shape)\n",
    "    print(np.array(ys).shape)\n",
    "    \n",
    "    #attributions_gradin = de.explain('grad*input', target_tensor, input_tensor, xs, ys=ys)\n",
    "    #attributions_sal   = de.explain('saliency', target_tensor, input_tensor, xs, ys=ys)\n",
    "    attributions_ig    = de.explain('intgrad', target_tensor, input_tensor, xs, ys=ys, steps=100)\n",
    "    attributions_dl    = de.explain('deeplift', target_tensor, input_tensor, xs, ys=ys)\n",
    "    attributions_idl   = de.explain('idl_true', target_tensor, input_tensor, xs, ys=ys, steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.mean(np.abs(attributions_dl) - np.abs(attributions_idl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(np.abs(attributions_ig) - np.abs(attributions_idl)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
